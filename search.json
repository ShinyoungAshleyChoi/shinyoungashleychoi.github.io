[

{
"title": "[알고리즘] 재귀, 조열, 순열, 정렬알고리즘",
"url": "/algorithm-01/",
"content": "알게된 내용 정리💡스터디 중 알게된 내용을 이론, 문법 중심으로 정리합니다.  재귀 함수          자기 자신을 호출하는 함수      Base Case: 재귀 함수를 종료하는 부분                  Base Case가 없다면 무한 뤂에 빠질 수 있다          Recursive Case의 마지막 부분이라고 생각하면 쉽게 작성 가능하다.                    Recursive Case: 자기 자신을 호출 하는 부분        조합과 순열          조합(Combination): 순서 X      순열(Permutation):  순서 O      조합과 순열의 관계: nCr에 의 모든 경우에 대해 나열(순서)하면 된다      재귀로 구현 가능        정렬 알고리즘          브루트포스: 모든 경우 살펴보기      그리디: 매순간 최선의 선택      DP: 복잡한 문제를 여러개의 문제로 나눔      문제 풀이💡문제 풀이를 정리합니다.  문제1 -  BOJ107          유형: #재귀 #피보나치      난이도(선택):      시간복잡도                  O(n^2)                    아이디어                  핵심 풀이 아이디어: 문제에 다 나와있지만 와닿지 않아서 숫자를 다 대입해봄                      사고 과정              1. 대입  	n == 0: 0,  	n == 1: 1  	n == 2: 0+1  	n == 3: 1+1 (2)  	n == 4: 1+2 (3)  	n == 5: 2+3 (5)  	n == 6: 5+3 (8) == f(5) + f(4) == (f(4)+f(3)) + (f(3)+f(2))  		== ((f(3)+f(2))+(f(2)+f(1))+(f(2)+f(1))+f(1)+f(0)  		== .....  		==  2. 규칙 발견  	F(n) = F(n-1) + F(n-2)  3. Base Case  	n==0, n==1                                엣지 케이스(선택):                    구현 순서              구현 코드          def fibo(n):  	if n &lt;= 1:  		return n  	return fibo(n-1) + fibo(n-2)                            회고: 메모이제이션을 사용하면 시간복잡도를 줄일 수 있다.          def fibo(x):  	global lst  	if lst[x] != -1:  		return lst[x]  	lst[x] = fibo(x-1) + fibo(n-2)  	return lst[x]  lst = [-1] * (n+2)  lst[0] = 0  lst[1] = 1  x = int(input())  print(fibo(x))  # O(n)                      문제2 - BOJ6603 (https://www.acmicpc.net/problem/6603)          유형: #조합 #재귀      난이도(선택):      시간복잡도                  최악의 경우 13C6 → O(n)                    아이디어                  핵심 풀이 아이디어: kC6, 재귀를 활용          사고 과정:                          Base Case: level == r              Recursive Case: 하나씩 뽑기 반복                                엣지 케이스(선택):                    구현 순서              구현 코드          def solution(index, level):  	global choose, lst, k  	if level == 6:  		for n in choose:  			print(n, end=' ')  		print()  		return  	for i in range(index, k):  		choose.append(lst[i])  		solution(i+1, level+1)  		choose.pop()  while True:  	choose = []  	I = list(map(int, input().split()))  	k = I[0]  	lst = I[1:]  	if k == 0:  		break  	solution(0, 0)  	print()                    회고:        문제3 - BOJ10974          유형: #순열 #재귀      난이도(선택):      시간복잡도                  O(NlogN)                    아이디어                  핵심 풀이 아이디어: 재귀를 활용          사고 과정:                          Base Case: N개를 고르는 경우              Recursive Case:                                엣지 케이스(선택):                    구현 순서              구현 코드          def solution(level):      global N, choose, check      if level == N:          print(' '.join(map(str, choose)))          return      for i in range(1, N + 1):          if check[i] is True:              continue          choose.append(i)          check[i] = True          solution(level + 1)          check[i] = False          choose.pop()  N = int(input())  choose = []  check = [False] * (N + 1)  solution(0)                    회고:        문제4 - BOJ11650          유형: #정렬      난이도(선택):      시간복잡도                  1 ≤ N ≤ 100 → NlogN &lt; 1억          O(NlogN)                    아이디어                  핵심 풀이 아이디어: 정렬          사고 과정:                          곱한 순으로 정렬 → 더한 순으로 정렬                                엣지 케이스(선택):                    구현 순서              구현 코드          N = int(input())  ranks = [list(map(int, input().split()))for _ in range(N)]  ranks = sorted(  	ranks,  	key=lambda x: (x[1]*x[2]*x[3], x[1]+x[2]+x[3], x[0])  )  for rank in ranks[:3]:  	print(rank[0], end=' ')                    회고: 입출력 형식 맞추는 것이 킹받는다      ",
"tags": ["algorithm"]
},

{
"title": "⭐️️⭐️️2025년 회고⭐️️⭐️️",
"url": "/retrospection-2025/",
"content": "2025년을 마무리 하며…❤️고난 뒤에 고난이 있나니, 버티는 것이 곧 실력이다!3월 즈음 번아웃이 왔다. 출근을 하고싶지 않았다. 사태의 악화를 막고자 팀장님에게 바로 커피챗을 요청했다.팀장님은 번아웃 극복 방안으로 운동을 추천해주셨고 나는 그 길로 당장 집 앞 헬스장에 PT를 등록했다.그리고 그날 이후 매일 팀장님과 아침 시간 짧은 커피챗을 나누며 멘탈을 다잡았다.그렇게 번아웃을 슬기롭게 이겨내고 잠시 동안의 해프닝으로 마무리 되는 줄 알았는데...6월의 어느 날, 밤마다 가슴을 조여 오는 고통이 시작됐다. 건강에 문제가 있다는 것을 본능적으로 알 수 있었다.그렇게 일반내과와 순환기내과를 오가며 원인을 찾았지만 일반인 정도 수준의 위염, 역류성 식도염 뿐이었다. 그마저도 약을 먹는 내내 증상은 호전될 기미가 없었다.고통의 심각성보다 원인을 모른다는 막막함이 눈 앞을 가렸다. 매일 밤 잠에 드는 것이 무서웠다.순환기 내과에서 혹시 몰라 처방해 준 니트로글리세린을 먹고 증상이 곧 바로 사라지는 경험을 할 때 까지는 말이다.병원에서는 변이형 협심증을 의심했고 확진을 위해 관상동맥 조영술까지 했는데 이상이 없다고 했다.의사 선생님에게 투약 후 바로 증상이 호전되었다고 하자, 검사의 정확도는 90%가량이라고 하며 꾸준히 협심증 약을 먹어보자고 했다.지금에서야 이 과정들이 너무 별 것도 아닌 것처럼 보이지만 당시의 나는 세상이 무너진 것처럼 패닉해버렸다.번아웃은 벗아난 줄 알았지만 멘탈은 회복되지 않은 상태였나보다.결국 나는 내 몸과 정신 건강을 지키기 위해 퇴사를 했다.퇴사 후의 삶은 꿀 맛같았고 두 달 간은 무너진 건강을 챙기며 보냈다. 하루에도 병원을 두 세 군데 방문 했으니 일을 할 때보다 스케줄이 더 많았다.자칫 밤낮이 바뀌어 버릴까봐 매일 새벽 5,6시에 일어나고 밤에는 일찍 자는 습관을 들였다. 운동과 공부도 멈추지 않았다.그렇게 푹 쉬었다는 생각이 들 때 쯤 이직 준비를 시작했다. 개발자 커리어를 시작한 후 첫 이직이었고, 나는 자신감에 차있었다.그런데 이게 무슨 일인지 첫 면접에서 완전 긴장해버렸고 면접도 망쳤다. 그 이후의 면접들에서도 기술 질문은 생각보다 날카로웠고 내가 가진 지식은 생각보다 얄팍했다.그 후로 면접 준비에만 올인하며 절치부심했다. 이 몇 개월이 어떻게 지나갔는지 기억이 나지 않는다. 수 많은 서류, 면접 탈락 끝에 4 곳에 최종 합격을 했고 그 중 한 곳으로의 오퍼를 수락해 이직에 성공했다.수 많은 면접 과정을 거치며 깨달은 건 경험적으로 알고 있는 노하우 + 의식적인 공부를 통해 얻은 교과서적 지식 + 일에 대한 태도 삼박자를 모두 갖춰야 한다는 것이었다.세 가지 중 한 가지만 모자라도 예측할 수 없는 면접관의 질문에 말문이 막히기 십상이다. 스파크, 아이스버그, 파이썬 모두 잘 안다고 생각했지만 면접을 위해 알고 있는 지식을 정리하고 말로 표현하는 연습을 계속 했다.아쉽지만 코딩 테스트나 과제가 있는 회사는 깔끔하게 포기했다. 이 다음 번 이직 때에는 자료구조와 알고리즘 공부까지 하면 더 좋겠다고 생각했다.한편으로는 내가 그동안 얼마나 게을렀는가, 그동안 '공부'라고 생각하며 쌓아왔던 지식들이 기록하지 않음으로서 얼마나 허망하게 사라졌는가를 생각하게 됐다.정말로... 말로 표현 못하게 힘들었던 한 해 였다. 번아웃부터 건강 이상과 이직 스트레스까지..하지만 이직 과정을 통해 성장 원동력의 복구와 회복 탄력성을 얻었으니 2026년에는 좀 더 나은 엔지니어가 될 수 있으리란 기대감이다.Good 👍  게으름 탈출  성장 원동력 회복Bad 👎  이직 과정에서의 스트레스 &amp; 재정 관리Summary갑각류 동물들은 탈피를 한 직후 가장 취약한 상태가 된다.하지만 그 과정이 없다면 성장도 없다.취약함을 드러내고 이겨내는 자만이 더 단단한 껍질을 얻는다.극악의 난이도였던 2025년, 하지만 덕분에 그만큼의 성장도 이뤄냈다!",
"tags": ["retrospection"]
},

{
"title": "Understanding Apache Iceberg (5) - Optimizing Iceberg tables",
"url": "/optimizing-iceberg-tables/",
"content": "이번 장에서는 아이스버그 테이블을 최적화 할 수 있는 방법에 대해 알아본다!Compaction  컴팩션은 ‘스몰 파일 문제’를 해결 하기 위해 자잘한 파일을 하나의 파일로 합쳐 IO와 스캔 효율을 높이는 것을 말한다.  데이터 계층과 메타데이터 계층 모두에 적용 가능  아래는 데이터 파일에 컴팩션을 적용하는 두 가지 방법    Table table = catalog.loadTable("myTable");SparkActions.get().rewriteDataFiles(table).sort() // 정렬 전략 사용 (뒤에서 자세히).filter(Expressions.and(  Expressions.greaterThanIrEqual("date", "2023-01-01"), // 1월 데이터만 필터링  Expressions.lessThanOrEqual("date", "2023-01-31"))).option("rewrite-job-order", "files-desc") // 큰 파일 그룹 먼저 컴팩션.execute();))        아래는 좀 더 쉬운 프로시져를 이용한 방법    CALL catalog.system.rewrite_data_files(  table =&gt; 'musicians',  strategy =&gt; 'binpack',  where =&gt; 'genre = "rock"'  options =&gt; map(      'rewrite-job-order', 'bytes-asc',      'target-file-size-bytes', '1073741824', -- 1GB      'max-file-group-size-bytes', '10737418240' --10GB, 하나의 비동기 스파크 태스크에서 총 10GB의 데이터를 처리  ))        컴팩션 전략들    Binpack    단순 파일만 합치기, 빠름    Sort    1개 혹은 여러 개의 필드를 대상으로 순차적으로 정렬  데이터가 쿼리패턴에 맞춰 클러스터링되므로 읽기 시간 줄어듦  binpack에 비해 컴팩션에 더 시간 걸림    z-order    여러 필드를 동일한 가중치로 정렬  쿼리가 여러 필드의 필터링을 한다면 읽기 성능 향상에 도움  binpack에 비해 컴팩션에 더 시간 걸림스트리밍 데이터를 위한 예제CALL catalog.system.rewrite_data_files(    table =&gt; 'streamingtable',    strategy =&gt; 'binpack', --빠른 컴팩션을 위해 빈팩 적용    where =&gt; 'created at between "2023-01-26 09:00:00" and "2023-01-26 09:59:59" ', --한시간 내 데이터에 적용    options =&gt; map(        'rewrite-job-order', 'bytes-asc',        'target-file-size-bytes', '1073741824',        'max-file-group-size-bytes', '10737418240',        'partial-progress-enabled', 'true' --파일을 여러 파일그룹으로 나눠 점진적으로하는 컴팩션 허용 (점차적으로 하는 방식)    ))Sorting  sorting 혹은 clustering  스캔해야할 데이터 파일의 범위를 줄일 수 있다  Z-orderPartitioningCopy-on-Write vs. Merge-on-Read기타 사항",
"tags": ["iceberg", "bigdata", "datalake"]
},

{
"title": "Understanding Apache Iceberg (4-2) - Lifecycle of Read Queries",
"url": "/lifecyle-of-queries-2/",
"content": "지난 포스트에 이어 이번에는 읽기 쿼리 과정을 살펴보자.읽기 쿼리시나리오1: select 쿼리SELECT *FROM ordersWHERE order_ts BETWEEN '2023-01-01' AND '2023-01-31'  엔진에 쿼리 보내기          쿼리 엔진이 메타데이터 파일들을 기반으로 실행 계획을 세울 준비를 한다.        카탈로그 확인          쿼리 엔진은 카탈로그에 orders 테이블의 현재 메타데이터 파일 경로를 요청하고 이를 읽는다.      쓰기 쿼리 수행 시와 마찬가지로 /orders/metadata/version-hint.txt (하둡 카탈로그 기준) 파일에서 3을 확인할 수 있다. (이전 포스트의 merge 쿼리 까지 실행한 상황)      이 정보를 바탕으로 엔진은 현재 메타데이터의 위치가 /orders/metadata/v3.metadata.json임을 알게 된다.        메타데이터 파일에서 정보 얻기          엔진은 해당 메타데이터 파일을 열어보고 아래와 같은 여러 정보를 얻는다.      테이블 스키마: 데이터를 읽기 위한 내부 메모리 구조를 준비하기 위해 사용      파티셔닝 전략: 데이터가 어떻게 정리되어 있는지 파악하기 위해 필요 - 불필요한 데이터 파일을 읽지 않을 수 있다.      현재 스냅샷 id(current-snapshot-id): 엔진은 스냅샷 어레이를 바탕으로 해당 스냡샷의 메니페스트 리스트의 경로 찾아내고, 그 경로를 따라가면서 관련된 파일들을 탐색하고 스캔한다.        매니페스트 리스트에서 정보 얻기          메타데이터 파일에서 매니페스트 리스트의 위치를 알고 나면 해당 매니페스트 리스트 파일을 읽고 더 많은 정보를 얻어 낸다.      해당 매니페스트 리스트 내의 각 스냡샷이 참조하는 매니페스트 파일의 위치: 이를 토대로 특정 쿼리에 관련된 데이터 파일을 읽을 수 있다.      파티션 관련 정보: partition-spec-id (특정 스냡샷에 사용된 파티션 전략), 파티션 단위 통계(파일 프루닝에 사용)      추가/삭제된 데이터 파일 개 수, 각 스냅샷 별 추가/삭제된 로우 개 수 등        매니페스트 파일에서 정보 얻기          이제 매니페스트 파일의 경로를 알고 있으니 프루닝 되지 않은 매니페스트 파일을 확인한다. 엔진은 메니페스트 파일에 기록된 각 엔트리를 스캔하는데 이는 매니페스트 파일이 추적하는 데이터 파일을 나타낸다.      쿼리 엔진은 각 데이터 파일이 속한 파티션 값을 우리 쿼리에 사용된 필터와 비교한다.      쿼리 조건은 WHERE order_ts BETWEEN '2023-01-01' AND '2023-01-31' 이었으므로 엔진은 2023-03-07-08과 같은 파티션 값은 무시한다. 필터값이 파티션 값과 일치하는 경우 해당 파티션의 모든 레코드를 확인한다.      파티셥 값을 기반으로 엔진은 해당 데이터 파일을 찾고 불필요한 파일을 스캔하지 않기 위해 다른 통계정보들도 모은다.      파티셔닝과 통계 기반 필터링(최대/최솟값등) 과 같은 데이터 및 파일 최적화 기술은 전체 테이블을 스캔하지 않음으로서 획기적인 성능을 보장해준다.      시나리오2: 타임 트래블 쿼리2편에서 공부했듯이.. 아이스버그의 강력한 무기 중 하나는 특정 스냡샷에 접근 할 수 있다는 것이다. 이를 통해 지난 분기의 데이터를 분석하거나 실수로 삭제된 행들을 복구할 수 있고 분석 결과를 재현할 수 있다.여기서는 4-1의 merge 쿼리를 실행하기 이전 상태를 복구할 필요가 있다고 가정해보자.SELECT * FROM catalog.db.orders.history;위 쿼리를 실행하면 해당 테이블의 현재까지의 모든 트랜잭션 리스트를 확인할 수 있다.해당 결과를 바탕으로 merge 쿼리 이전 상태를 반환하기 위해 아래와 같은 쿼리를 사용할 수 있다.            made_current_id      snapshot_id      parent_id      is_current_ancestor                  2023-03-06 21:28:35.360      7327164675870333694      null      true              2023-03-07 20:45:08.914      8333017788700497002      7327164675870333694      true              …      …      …      …      SELECT * FROM ordersTIMESTAMP AS OF '2023-03-07 20:45:08.914' --made_current_at 기준         SELECT * FROM ordersVERSION AS OF 8333017788700497002 -- snapshot_id 기준위 쿼리를 사용하면 2023-03-07의 상태인 merge 쿼리 이전의 상태에 접근할 수 있다.  엔진에 쿼리 보내기  카탈로그 확인하기          카탈로그에 현재 메타데이터 파일의 위치를 조회하고 이를 읽는다.      /orderes/metadata/version-hint.txt 파일에서 숫자 3을 확인하고 /orders/metadata/v3.metadata.json 파일을 확인한다. 이를 통해 파티션 전략과 테이블 스키마 정보를 확인한다.        메타데이터 파일에서 정보 얻기          현재 메타데이터 파일은 우리 아이스버그 테이블에서 생성된 모든 스냡샷을 추적하고 있다. (의도적으로 만료시킨 스냡샷이 아니라면!)      해당 스냡샷 리스트에서 엔진은 타임트래블 쿼리에서 특정된 스냡샷을 확인할 수 있다.      해당 스냡샷을 확인했으니 해당 특 스냅샷이 추적하는 매니페스트 리스트 경로를 알 수 있다.        매니페스트 리스트에서 정보 얻기          해당 매니페스트 리스트가 포함하는 매니페스트 파일들의 위치      추가/삭제된 데이터 파일 개 수      파티션 통계 정보        매니페스트 파일에서 정보 얻기          데이터 파일 경로      각 컬럼들의 통계 정보        데이터 파일 읽기          해당 데이터 파일을 읽는다.      ",
"tags": ["iceberg", "bigdata", "datalake"]
},

{
"title": "Understanding Apache Iceberg (4-1) - Lifecycle of Write Queries",
"url": "/lifecycle-of-queries/",
"content": "이 글의 내용은 Apache Iceberg: The Definite Guide (O’Reily)의 내용을 정리한 것 입니다.사족을 붙이자면, 이 부분을 어떻게 정리하는게 좋을까 하다가 그냥 읽고만 넘어갔었고 기억은 자연스레 휘발되었다..결국 면접에서 해당 부분에 대한 질문에 대답을 못한 기념으로 아무렇게나 정리해본다…..^^Recap of Architecture of Apache Iceberg                              Apache Iceberg의 컴포넌트      지난 포스트를 쓴지 오래 되었으니 복습부터 시작해보자!카탈로그 레이어  카탈로그는 현(최신) 시점의 각 테이블의 메타데이터 파일을 가리킨다.  읽기든 쓰기든 첫 시작은 바로 이 카탈로그에서 최신 테이블의 상태를 확인하는 것으로 시작된다.메타데이터 레이어  쿼리 엔진이 아이스버그 테이블에 뭔가를 쓸 때, 메타데이터 파일이 원자적으로 생성되며 최신 상태로 갱신된다. 이는 동시 쓰기와 같은 상황에서 테이블 커밋 기록을 선형적으로 유지할 수 있게 해준다.  읽기 시에는 쿼리 엔진은 항상 테이블의 최신버전을 바라보게 되는데, 이때 매니페스트 리스트를 통해 현재 파티션 스펙을 확인해서 불필요한 매니페스트 파일을 건너 뛸 수 있도록 한다.  매니페스트 파일의 통계정 보를 통해 파일 프루닝이 가능하다.데이터 레이어  읽기 시에는 쿼리 엔진이 메타데이터 파일을 통해 어떤 데이터 파일을 읽을 것인지 거른다.  쓰기 시에는 저장소에 데이터 파일을 쓰고 그에 따라 메타데이터 파일들이 생성되고 업데이트 된다.아이스버그의 쓰기 쿼리들!  쓰기 동작은 기본적으로 아래 프로세스를 거친다.                                                                    Apache Iceberg 쓰기 프로세스                      엔진에서 쿼리 파싱  일관성과 무결성을 유지 + 정해진 파티션 전략에 따라 데이터 쓰기 위해 카탈로그를 참조  데이터 파일과 메타데이터 파일이 쿼리 기반으로 쓰여짐      카탈로그 파일이 최신 메타데이터를 반영해 업데이트됨, 이후의 읽기 동작 시 최신 데이터를 바라볼 수 있도록.    여러가지 시나리오들을 통해 더 들여자 보자.시나리오1: 테이블 생성CREATE TABLE orders (    order_id BIGINT,  customer_id BIGINT,  order_amount DECIMAL(10, 2),  order_ts TIMESTAMP)USING icebergPARTITIONED BY (HOUR(order_ts))  모든 쿼리는 spark sql 기준  엔진에 쿼리 보내기          쿼리 엔진에서 쿼리를 파싱      이 경우 테이블 생성 쿼리이므로 엔진은 테이블을 생성하고 정의하겠쥬 (당연한 말)        메타데이터 파일 쓰기          테이블 경로에 v1.metadata.json 이라는 제목의 메터데이터 파일을 생성 시작      이 메타데이터 파일에서 orders 테이블의 스키마를 정의      table-uuid 필드에 유니크한 식별자 저장      현재는 실 데이터가 없기 때문에 해당 메타데이터는 추적할 매니페스트 리스트도, 파일도 없음      아래는 우리가 작성한 테이블 생성문에 따른 최초의 메타데이터 파일 내용!         {"table-uuid": "072db680-d810-49ac-935c-56e901cad686","schema": { "type": "struct", "schema-id": 0, "fields": [{ "id": 1, "name": "order_id", "required": false, "type": "long" }, { "id": 2, "name": "customer_id", "required": false, "type": "long" }, { "id": 3, "name": "order_amount", "required": false, "type": "decimal(10, 2)" }, { "id": 4, "name": "order_ts", "required": false, "type": "timestamptz" }], "partition-spec": [{ "name": "order_ts_hour", "transform": "hour", "source-id": 4, "field-id": 1000 }]} }                       변경 사항을 저장하기 위해 카탈로그 파일 업데이트 하기          쿼리 엔진이 카탈로그 파일인 version-hint.text에 현재 시점의 메타데이터 포인터를 v1.metadata.json으로 업데이트      시나리오2: 쓰기 쿼리INSERT INTO orders VALUES (                           123,                           456,                           36.17,                           '2023-03-07 08:10:23'                          )  엔진에 쿼리 보내기 (쿼리 파싱)  카탈로그 확인          쿼리 엔진이 가장 처음 하는 일은 카탈로그에 현재 메타데이터 파일의 위치를 요청하고 그것을 읽는 것!      엔진은 /orders/metadata/version-hint.txt 의 파일 내용인 1을 확인한다. (경로는 하둡 카탈로그 기준)      2번을 통해 엔진은 현재 메타데이터 파일의 위치가 /orders/metadata/v1.metadata.json임을 알게되고 이를 읽는다.        데이터 파일과 메타데이터 파일 쓰기          엔진이 스키마와 파티셔닝 정보를 알게 되면 새로운 데이터 파일과 이와 관련된 메타데이터 파일을 작성하기 시작한다.      데이터 파일을 작성하고 나서 엔진은 매니페스트 파일을 생성한다. 이 매니페스트 파일(avro파일)은 실제 데이터 파일의 경로에 대한 정보, 통계 정보를 담고 있다.      매니페스트 파일 예시는 이전 글을 참고해보세요..      다음으로는 메니페스트 파일을 추적하는 매니페스트 리스트를 생성한다. 이미 현재 스냅샷의 매니페스트 파일이 존재한다면 이 파일들은 새로운 매니페스트 리스트에 추가된다.      매니페스트 리스트는 데이터 파일의 개수, 추가되거나 삭제된 로우 수, 파티션에 대한 통계정보등을 담는다.      매니페스트 리스트 예시도 이전 글의 링크를 참조..      마지막으로 엔진은 새로운 스냡샷 s1과 함께, 새로운 메타데이터 파일, 즉, v2.metadata.json을 생성한다. 이전의 v1.metadata.json은 s0 스냡샷을 추적했다.      이 새로운 메타데이터 파일은 매니페스트 리스트 위치, 스냡샷 ID, 오퍼레이션 요약 정보 등 매니페스트 리스트에 대한 정보를 담고 있다.      메타데이터 파일 예시도…..예        변경 사항 저장을 위한 카탈로그 파일 업데이트 하기          카탈로그로 다시 돌아가, 이 쓰기 작업 수행 시에 동안 다른 스냡샷이 커밋되지는 않았는지 확인한다. - 아이스버그에서 동시 쓰기를 보장하는데에 핵심인 부분..!      낙관적 동시성 제어에 따라 아이스버그는 쓰기 커밋이 이루어지기 전까지 쓰기 충돌이 없다고 가정하고 새 메타데이터 파일을 생성한다.      새 버전의 메타데이터 파일로 원자적으로 포인터를 스위치한다.      시나리오3: Merge 쿼리 (upsert)sqlMERGE INTO orders oUSING (SELECT * FROM orders_staging) sON o.order_id = s.,order_idWHEN MATCHED THEN UPDATE SET order_amount = s.order_amountWHEN NOT MATCHED THEN INSERT *  엔진에 쿼리 보내기 (쿼리 파싱)          쿼리에 있는 두 개의 테이블에 대해 실행 계획을 세움        카탈로그 확인          위 쓰기 쿼리와 마찬가지로 카탈로그에 현재 메타데이터 파일의 위치를 확인하고 그 파일을 읽음      version-hint.txt에서 정수값 2를 확인 한다. 즉, 현재 메타데이터 파일의 위치는 /orders/metadata/v2.metadata.json임을 알게된다.      이 메타데이터 파일을 통해 테이블의 최신 스키마를 확인하고 쓰기 작업이 스키마에 맞게 수행되도록 한다.      마지막으로 엔진은 파티셔닝 전략에 따라 데이터 파일들이 어떻게 구성되어 있는지를 파악하고 데이터 파일 쓰기를 시작한다.        데이터 파일과 메타데이터 파일 쓰기          orders_staging과 orders테이블의 데이터를 메모리에 로드하여 서로 일치하는 레코드를 찾는다.      아이스버그는 두가지 쓰기 전략 즉, COW(copy-on-write)와 MOR(merge-on-read)를 사용한다.                  COW는 아이스버그 테이블 갱신시 관련된 데이터 파일을 새로 작성한다. (높은 쓰기 오버에드/낮은 읽기 오버헤드)          MOR전략에서는 기존 파일을 다시 쓰지 않고 변경된 내용을 추적하기 위한 새로운 delete 파일을 생성한다. (낮은 쓰기 오버헤드/높은 읽기 오버헤드)                    이번 예제에서는 COW전략을 사용한다고 가정.      orders테이블에 있는 order_id = 123 을 포함한 데이터 파일 0_0_0.parquet 을 메모리로 읽어온다.      order_staging 테이블의 새로운 order_amount 값으로 order_id = 123의 order_amount필드를 갱신한다.      수정된 결과는 새로운 parquet파일로 기록된다.      COW 전략을 사용한다고 가정했기 떄문에 쿼리 조건과 일치 하지 않는 레코드들 또한 새 데이터 파일에 (변경없이) 기록된다.      order_staging테이블에서 조건에 맞지 않았던 레코드는 일반적인 INSERT로 처리 되어 새로운 파티션에 새로운 데이터 파일로 작성된다.      데이터 파일을 작성한 후 엔진은 두 개의 데이터 파일 경로를 참조하는 새로운 메니페스트 파일을 생성한다. (메니페스트 파일은 뭐다? 데이터 파일의 통계정보를 담은 파일!)      이후 엔진은 방금 생성한 메니페스트 파일을 가리키는 새로운 매니페스트 리스트를 생성한다. 이 리스트에는 기존의 메니세프슽 파일들도 함꼐 추적된다. (메니페스트 리스트에는 파티션 통계나 추가/삭제된 파일의 수 같은 정보 포함)      그 후 엔진은 새로운 스냅샷(s2)를 포함하는 새로운 메타데이터 파일 v3.metadata.json을 생성한다.      이 파일은 이전 메타데이터 파일인 v2.metadata.json을 기반으로 하며 그 안에는 이전 스냡샷 (s0, s1)도 함꼐 포함된다.        변경 사항 저장을 위한 카탈로그 파일 업데이트          마지막으로 엔진은 다시한 번 쓰기 충돌이 없는지 겅증한 뒤 최신 메타데이터 파일 (v3.metadata.json)로 카탈로그를 업데이트 한다.                                    Merge Into 실행한 결과      ",
"tags": ["iceberg", "bigdata", "datalake"]
},

{
"title": "[토이프로젝트] MCP로 AI에 내 헬스 데이터 연결하기! (기획 &amp; 간단 설계)",
"url": "/MCP-health-data-project-planning/",
"content": "MCP로 AI에 내 헬스 데이터 연결하기!1. 프로젝트 개요  프로젝트명: Real-time Health Data MCP Server (AI Agent Query Focus)  배경 및 필요성          스마트폰과 웨어러블을 통한 개인 헬스 데이터 수집이 일상화되면서, AI가 실시간으로 데이터에 접근하여 분석·피드백을 주는 서비스 수요 증가.      기존 헬스 데이터 분석은 배치 처리 중심이라, 즉각적인 사용자 질의응답에 제약이 있음.        목표          iOS HealthKit과 MCP 서버를 연동해 실시간 헬스 데이터를 안전하게 수집하고, AI Agent가 대화 중에 바로 질의·응답할 수 있는 환경을 구축.      2. 주요 기능  실시간 헬스 데이터 수집 (걸음수, 심박수, 수면, 운동)  이벤트 기반 증분 동기화 (Observer Query + Anchored Query)  안전한 데이터 전송 (HTTPS/mTLS, Token 인증)  원본 데이터 Parquet 저장 (일 단위 파티션)  DuckDB 기반 빠른 질의 처리 (최근 데이터 위주)  MCP Tools: getSteps, getWorkouts, getHeartRateStats  MCP Resources: weekly_report.md(주간 리포트), 일간 요약 Parquet3. 시스템 아키텍처      데이터 흐름:    iOS 브리지 앱 → HTTPS/mTLS 전송 → MCP 서버 → Parquet 저장 → DuckDB 질의 → MCP Tools/Resources → AI Agent    저장 구조:          bronze/quantity_events/dt=YYYY-MM-DD/*.parquet      bronze/workouts/dt=YYYY-MM-DD/*.parquet        쿼리 처리 방식:          최근 30일 캐시(Parquet) → DuckDB → 응답      범위 초과 시 직접 Parquet 스캔      4. 기술 스택  클라이언트(iOS): Swift/SwiftUI, HealthKit API  서버: Python + FastAPI, Node.js, @modelcontextprotocol/sdk  저장소: Parquet (S3 또는 로컬), DuckDB  보안: TLS/mTLS, Bearer Token 인증  기타: Docker, GitHub Actions(CI/CD)5. 데이터 모델  Quantity 데이터: 걸음수, 심박수 등 (value, unit, start_ts, end_ts, uuid)  Category 데이터: 수면 분석, 활동 상태 등 (value, start_ts, end_ts, uuid)  Workout 데이터: 운동 타입, 거리, 시간, 칼로리 등 (activity, distance, duration, energy)  메타데이터: uuid, ingest_ts, device_id, dt 파티션6. 보안 및 개인정보 보호 정책  HealthKit 권한 최소화 (필요 데이터 타입만 요청)  모든 전송 구간 TLS/mTLS 암호화  Token 기반 인증 및 Rate Limit 적용  민감 데이터 로깅 금지  삭제 이벤트 수신 시 Parquet에서 삭제 반영7. 활용 시나리오  AI 대화형 질의: “지난 7일 걸음수 평균 보여줘” → MCP Tool 호출 → DuckDB 응답  운동 분석: “최근 러닝 워크아웃 요약해줘” → Parquet 필터링 후 결과 반환  건강 리포트 자동 생성: 주간 데이터 집계 후 weekly_report.md 생성8. 프로젝트 일정—9. 확장 방안1. 데이터 레이어 확장 (Bronze → Silver → Gold)  Bronze Layer (원본 저장)          HealthKit에서 수집한 원본 데이터를 Parquet로 일 단위 파티션 저장      중복/결측 포함, 원본 그대로 유지 → 재처리, 오류 복구 가능        Silver Layer (정제·표준화)          Iceberg 테이블로 관리, 스키마 표준화, 단위 변환, 다중 기기 데이터 병합      ACID 트랜잭션 및 스냅샷 기능으로 데이터 정합성 확보        Gold Layer (집계·최종 분석)          Airflow ETL로 주간/월간 통계 생성 (예: daily_steps, weekly_hr_stats)      AI Agent 및 대시보드가 즉시 사용할 수 있는 형태로 제공      2. 기술 확장  배치 파이프라인 도입          Apache Airflow + PySpark 기반 ETL로 실버/골드 테이블 갱신      스케줄러를 통한 자동화 및 모니터링        장기 분석 지원          Iceberg를 통한 수년치 데이터 관리 및 schema evolution 지원      장기 추세 분석, 사용자 맞춤형 건강 코칭 모델 가능        실시간 + 배치 하이브리드          현재 실시간 MCP Tools 기반 질의에, 배치 기반 MCP Resources를 결합      최신 데이터 + 누적 통계 모두 대응 가능        이상 감지 시 알림 기능 추가          규칙: 절대 임계, 급상승, 급격 변화, 휴지기(쿨다운)      채널: APNs + 카톡 + Email(백업)      운영: 개인 임계값 학습, 오탐 감소 로직, 일시중지 토글      ",
"tags": ["mcp", "vibe-coding", "real-time"]
},

{
"title": "Understanding Apache Iceberg (3) - Architecture",
"url": "/architecture-of-apache-iceberg/",
"content": "이 글의 내용은 Apache Iceberg: The Definite Guide (O’Reilly)의 내용을 정리한 것 입니다.Apache Iceberg의 아키텍쳐데이터 레이어(The Data Layer)  테이블의 실제 데이터를 저장하며 삭제 파일을 포함  데이터 레이어 의 파일은 Apache Iceberg 테이블의 트리 구조의 리프 노드들로 구성되어 있음  분산 파일 시스템 기반 (HDFS, Amazon S3, ADLS, GCS, …)데이터 파일(Datafiles)  말 그대로 실제 데이터를 저장하는 파일  여러 가지 파일 포맷을 지원, 내장 기능으로는 Apache Parquet, Apache ORC, Apache Avro를 지원          필요에 따라 적합한 파일 포맷을 이용하면 됨 (eg. 대규모 OLAP 분석 - parquet, 스트리밍 분석 테이블 - avro)      필요에 의해 파일 포맷이 변경되어 여러 파일 포맷이 혼재되어도 그대로 사용 가능      새로운 개선된 파일 포맷이 개발된다면 이를 적용할 수도 있음        여러 파일 포맷을 지원하지만 사실상 표준이 되는 것은 parquet 형식임          이는 컬럼 기반 구조가 행 기반 구조보다 대규모 OLAP에 적합하기 떄문      그 밖에, 하나의 파일을 여러 방식으로 분할해서 병렬성을 높일 수 있음      각 분할 지점에 대한 통계 정보를 가질 수 있어서 필터 푸쉬다운이나 스킵리드가 가능      압축 효율이 좋아서 저장 공간을 적게 차지하고 읽기 속도가 높아지는 이점이 있음      Parquet 파일의 내부 구조                              parquet 파일 아키텍쳐1        Row Group 0 은 하나의 로우 집합이며 각 컬럼에 대응되는 행들의 값이 모여 있는 집합으로 구성되며 이는 다시 페이지라는 단위로 분할 된다. 이렇게 나뉜 각 계층은 엔진이나 툴이 독립적으로 읽어올 수 있다  각 row group 에 대한 통계 정보(e.g., 어느 컬럼의 최소값, 최대값 등)을 가지고 있다. 이 통계 정보를 기반으로 쿼리 실행 시 해당 row group을 읽을지 말지 결정할 수 있다.삭제 파일(Delete Files)  삭제 파일은 데이터셋에서 어느 레코드가 삭제되었는지를 추적한다.  레코드가 삭제되면 변경분이 반영된 새 파일이 작성되거나 (copy-on-write [COW])  변경 분만을 기록한 새 파일이 작성되고 데이터를 읽을 때 이를 병합해서 보여줄 수 있다(merge-on-read [MOR])  수정/삭제 성능을 위해 MOR 방식을 취한다.  위치 삭제 파일(Positional delete files): 삭제된 행의 위치를 식별 하여 어떤 행이 논리적으로 삭제되었는지를 나타냄  균등 삭제 파일(Equality delete files): 컬럼 값 기반 삭제, 특정 조건 (특히 pk)에 해당하는 행을 논리적으로 삭제 처리하여 나타냄          그렇다면.. 어떤 조건에 해당하는 행을 삭제 한 후 새롭게 추가된 행이 그 조건에 해당한다면 어떻게 될까? 이 새로운 레코드 역시 삭제 처리되는 건 아닐까?      이를 해결 하기 위해 시퀀스 넘버를 활용한다. 모든 파일(data file, delete file)에는 고유한 시퀀스 번호가 할당되고 균등 삭제 파일은 자신보다 같거나 작은 수의 data file에만 적용함      메타데이터 레이어(The Metadata Layer)매니페스트 파일(Manifest Files)  매니페스트 파일은 데이터 레이어의 파일들(datafiles/delete files)과 각 파일의 추가적인 세부사항과 통계 정보들을 추적한다.  Hive와 구별되는 Iceberg의 특징이 바로 파일 레벨에서 어떤 데이터가 속해있는지를 추적한다는 것이다. (디렉터리 레벨이 아니라!)  매니페스트 파일은 메타데이터 트리의 말단 노드 레벨에서 이를 수행하는 파일이다.  매니페스트 파일의 내용은 해당 데이터 파일의 경로, 포맷, 어느 파티션에 속해 있는지, 레코드 개수, 컬럼의 최솟값 및 최댓값등을 포함  즉, 통계 데이터를 위해 데이터 파일을 열 필요가 줄어듦. -&gt; 성능 향상  실제 매니페스트 파일의 실제 내용을 보면 이해가 더 쉬웠음!  메니페스트 파일 예시 (실제로는 avro 포맷이지만 편의상 json으로 변환한 것)메니페스트 리스트(Manifest Lists)  매니페스트 리스트는 어떤 주어진 시점의 Iceberg 테이블의 스냅샷을 의미  해당 시점의 매니페스트 파일들과 해당 매니페스트 파일이 참조하는 데이터 파일의 위치, 속한 파티션, 파티션 컬럼의 최댓값, 최솟값을 포함  매니페스트 리스트는 어레이로 감싼 스트럭트 타입을 포함하며 각 스트럭트는 단일 매니페스트 파일을 추적한다.  매니페스트 리스트 예시 (실제로는 avro 포맷이지만 편의상 json으로 변환한 것)메타데이터 파일(Metadata Files)  메타데이터 파일은 메니페스트 리스트를 추적한다.  메타데이터 파일은 특정 시점의 Iceberg 테이블에 관한 메타데이터를 저장한다.  각 시점 마다 변경이 이루어지면 메타데이터 파일이 생성되고 카탈로그에 의해 원자적으로 최신 메타데이터 파일로 등록된다.  테이블 커밋 히스토리는 선형적이며 다중 쓰기에 도움을 준다.  메타데이터 파일 예시퍼핀 파일(Puffin Files)  퍼핀 파일은 좀 더 넓은 범위의 쿼리의 성능을 향상시키기 위해 데이터에 관련된 통계정보와 인덱스들을 저장한다.  퍼핀 파일은 블롭(blob)이라는 임의의 시퀀스 셋과 블롭을 분석하는데 필요한 메타데이터가 포함되어 있다.  현재는 Apache DataSketches 라이브러리의 Theta sketch(bloom filter와 같은 확률적 알고리즘-근사 알고리즘의 일종) 타입만을 지원  Theta sketch는 주어진 행 집합에서 특정 컬럼의 고유 값 개수를 근사 계산할 수 있는 알고리즘이다. 연산 속도가 빠르고 리소스를 적게 쓸 수 있다.  정확한 값을 구하는데 비용이나 시간이 너무 많이 드는 경우, 동일한 연산을 반복해서 실행하는 경우 (대시보드)에 유용카탈로그(Catalog)  Iceberg 테이블의 현재 메타데이터 파일 위치를 추적하고 읽기/쓰기 엔진이 테이블의 최신 상태를 알게 해준다.  메타데이터 파일 경로를 찾으면 테이블 상태 (스키마, 스냅샷 등)을 확인 가능  SQL쿼리로 최신 메타데이터 경로 조회 가능      SELECT *FROM my_catalog.iceberg_book.orders.metadata_log_entriesORDER BY timestamp DESCLIMIT 1                  https://parquet.apache.org/docs/file-format/ &#8617;      ",
"tags": ["iceberg", "bigdata", "datalake"]
},

{
"title": "Understanding Apache Iceberg (2) - Key Features",
"url": "/key-features-of-apache-iceberg/",
"content": "이 글의 내용은 Apache Iceberg: The Definite Guide (O’Reilly)의 내용을 정리한 것 입니다.Apache Iceberg의 주요 특징ACID 트랜잭션  ACID 보장을 위해 낙관적 동시성(optimistic concurrency) 적용  낙관적 동시성(optimistic concurrency)은 락(lock)의 최소화와 성능 향상을 위해 트랜잭션들이 서로 충돌하지 않음을 가정하며 필요할 때만 충돌을 확인한다. 커밋에 성공하거나 실패하거나 둘 중 한가지 상태만 존재한다.  비관적 동시성(pessimistic concurrency)모델에서는 충돌이 일어날 것을 가정하여 트랜잭션 간 충돌을 방지하기 위한 락을 사용한다. 이는 현 시점에서는 Apache Iceberg에 적용 불가능하나 추후에는 가능할 수도 있다.  동시성 보장은 카탈로그(catalog - (3) Architecture 편에서 자세히)가 다룬다.파티션 진화 (Partition evolution)  Apache Iceberg 이전의 데이터 레이크 환경에서의 큰 골칫거리 중 하나는 태이블의 물리적 최적화 변경  파티셔닝을 변경해야 할 때 선택 가능한 유일한 방법은 전체 테이블을 다시 쓰는 것, 아니면 현재 파티셔닝을 유지하면서 잠재적 성능 향상을 희생하는 것이었음  Apache Iceberg에서는 파티셔닝 변경을 위해 메타데이터만 수정되면 되므로 테이블과 전체 데이터를 다시 쓰지 않고 언제든 파티셔닝을 업데이트 할 수 있음      아래 그림에서, 파티셔닝은 처음에는 month를 기준으로 되었다가 day기준으로 변경됨, 이전 파티셔닝과 변경된 파티셔닝이 적용된 데이터를 모두 가져올 때 파티셔닝 방식에 따라 실행계획이 분리됨                                                                  그림11                    숨겨진 파티셔닝 (Hidden Partitioning)  Hive나 전통적인 시스템에서는 timestamp 컬럼으로 파티셔닝 하면, 내부적으로는 event_year, event_month, event_day같은 식으로 다른 컬럼들이 생성되어 파티셔닝됨  event_timestamp &gt;= DATE_SUB(CURRENT_DATE, INTERVAL 90 DAY)  Hive에서 최근 90일간의 평균 수익을 얻기 위해 위와 같은 필터링을 한다면, event_year, event_month, event_day를 직접 필터링하는게 아니기 때문에 전체 스캔을 하게 됨  하지만 Apache Iceberg에서는 파티셔닝 컬럼을 직접 쿼리할 필요가 없도록 내부에서 자동으로 처리해줌행 기반 테이블 운영 (Row-level table operations)  Apache Iceberg에서는 행 단위 업데이트를 copy-on-write(COW) 혹은 merge-on-read(MOR) 방식으로 처리  Copy-on-write(COW): 한 행이라도 변경 발생시 전체 파일을 새로 씀  Merge-on-read(MOR)          변경된 행들만 새로운 파일에 저장해두고 읽을 때 최신상태로 합쳐서 보여줌      무거운 수정&amp;삭제 작업을 가볍게 처리 가능      타임 트래블  Apache Iceberg는 불변 스냅샷을 제공하여 테이블의 히스토리컬 상태에 접근 가능하며 과거 시점을 기준으로 쿼리를 실행할 수 있음버전 롤백  물론 과거 시점의 데이터에 접근하는 것 뿐만 아니라 해당 시점의 스냅샷으로 테이블을 되돌릴 수도 있음스키마 진화 (Schema evolution)  컬럼 추가/삭제, 컬럼 이름 변경, 컬럼 데이터 타입 변경등이 가능            https://www.dremio.com/blog/future-proof-partitioning-and-fewer-table-rewrites-with-apache-iceberg/ &#8617;      ",
"tags": ["iceberg", "bigdata", "datalake"]
},

{
"title": "Understanding Apache Iceberg (1) - Introduction",
"url": "/introduction-of-apache-iceberg/",
"content": "이 글의 내용은 Apache Iceberg: The Definite Guide (O’Reilly)의 내용을 정리한 것 입니다.Apache Iceberg 소개  성능 및 일관성 외 여러 Hive의 약점들을 극복하기 위해 Netflix에서 개발한 테이블 포맷테이블 포맷이란? - “What data is in this table?”  A table format is a method of structuring a dataset’s files to present them as a unified “table”. From the user’s perspective, it can be defined as the answer to the question “what data is in this table?”1  단 위의 내용을 직역하여 단순히 “이 테이블에는 어떤 데이터가 들어있는가?”로 이해한다면 오해의 여지가 있음. - 기존의 전통적인 방식의 RDB에서처럼 employees 테이블에 (홍길동, 영업팀) 따위의 레코드가 들어있다는 것으로 오해하기 쉬음.  과거에는 각 데이터베이스의 저장 엔진이 데이터의 읽기/쓰기를 단독적으로 관리했지만 빅데이터 시대가 열린 이후로 이 방법은 더이상 실용적이지 않게 됨  데이터 레이크 시대에는 데이터가 대용량 저장 솔루션(e.g. AWS s3, Azure Data Lake Storage, Google Cloud Storage)에 파일 형태로 존재하며 단일 테이블은 이러한 수백, 수천, 수백만의 개별 파일들로 이루어져있음.  이러한 데이터 레이크 관점에서 “이 테이블이 참조하고 있는 파일들에는 각각 어떤 데이터가 포함되어 있는가?”로 이해하는 것이 맞다고 보임.  어떤 sql이나 애드혹 스크립트를 실행하기 위해 어떤 파일에 어떤 데이터가 들어 있는지를 일일이 파악하고 싶지는 않을 것.  다른 말로 표현하면, 데이터셋의 파일들을 통일된 테이블로 표현하기 위해 구조화하는 방법Iceberg 이전의 세계에는 Hive가 존재Hive의 특징  하둡 데이터 레이크 하에서는 데이터 분석에 맵리듀스 프레임워크를 사용, 사용자들은 복잡하고 지루한 자바 작업을 해야했음  맵리듀스 잡 대신 SQL을 사용하여 편리하게 데이터 분석을 할 수 있도록 등장한 것이 Hive.  Hive는 SQL을 실행 가능한 맵리듀스 잡으로 변환  하지만 SQL문을 작성할때도, Hadoop 스토리지에 저장된 데이터 중 어떤 게 고유한 테이블을 나타내는지 알 수 있는 메커니즘이 필요, 이것이 Hive 테이블 포맷과 테이블 정보를 관리, 추적하는 Hive Metastore가 탄생한 이유.  Hive 테이블 포맷은 지정된 디렉터리에 들어 있는 모든 파일을 하나로 봄          그 디렉터리 안의 하위 디렉터리가 곧 파티션이 됨      이 디렉터리 경로 전체를 Hive Metastore 가 기록하고 관리.      Hive의 장점  파티션이나 버켓팅같은 기술 덕분에 풀 스캔없이 필요한 데이터만 빠르게 조회 가능  파일 포맷에 독립적 - Parquet, Avro, CSV/TSV같은 다양한 포맷의 파일을 읽기 가능  Hive Metastore에 리스팅된 디렉터리의 원자적 스왑을 통해 테이블의 개별 파티션 단위로 원자적 변경이 가능.Hive의 단점  파일 레벨의 변경은 비호율적. 파티션 디렉터리를 변경하는 것처럼 원자적으로 파일을 바꿀 수 있는 메커니즘은 없었음  파티션을 원자적으로 스왑할 때, 한 트랜잭션에 여러 파티션을 원자적으로 업데이트 할 수 있는 메커니즘은 없었기 때문에 여러 파티션을 업데이트 하는 동안에는 일관성이 깨질 수 있음  동시 업데이트를 가능하게 하는 메커니즘이 존재하지 않음 (사실 Hive를 뛰어넘는 툴들에서도!)  파일과 디렉터리 리스트를 나열하는데 시간이 소요되고 쿼리를 느리게 만듦, 스캐닝이 필요 없는 파일이나 디렉터리까지 리스팅하기 때문에 쿼리 결과를 얻는데 추가적인 비용 소모.  파티션 컬럼은 종종 다른 컬럼에서 파생되어 만들어짐. - 타임스탬프로부터 월 컬럼을 만드는 경우, 파티션 컬럼으로 필터링할 경우에는 파티셔닝이 도움이 되지만 타임스탬프 컬럼으로 필터링하는 쿼리는 파생된 월컬럼을 함께 필터링 해야 한다는 사실을 직관적으로 알기 어려워 전체 테이블을 스캔하는 문제 발생.  테이블 통계는 비동기적으로 수집되어 통계 정보가 없거나 오래된 경우 발생, 쿼리 엔진이 추가적인 최적화를 수행하는데 어려움을 겪음  데이터 셋과 사용 사례의 규모가 커질 수록 위 문제들은 더욱 심각해짐현대 데이터 레이크 테이블 포맷  현대 데이터 레이크 테이블 포맷의 창시자들은 Hive 테이블 포맷이 겪는 문제의 원인은 개별 파일이 아닌 디렉터리 내용 기반의 테이블 정의라는 것을 깨달음  Apache Iceberg, Apache Hudi, Delta Lake와 같은 현대 테이블 포맷들은 모두 이러한 접근 법을 따름, 메타데이터는 “어떤 파일들이(디렉터리가 아니라!) 테이블을 구성하느냐”를 엔진에게 알려줌  이러한 접근 법은 ACID 트랜잭션, 동시 쓰기 중 일관성 유지, 더 나은 통계 정보와 메타데이터 제공, 타임 트레블 등과 같은 새로운 세계로의 문을 열어줌Iceberg의 목표  일관성(Consistency): 여러 파티션에 걸쳐 데이터가 업데이트 되는 경우, 이는 빠르고 원자적으로 이루어져야 하며 사용자는 업데이트 전 혹은 후 상태만을 볼 수 있어야 한다. 즉, 어떤 파티션에는 업데이트가 적용되어 있고, 어떤 파티션에는 적용이 되지않은 중간(일관성이 깨진) 상태를 목격해서는 안된다.  성능(Performance): 테이블은 메타데이터를 제공하고 과도한 파일 나열을 피해야함, 그렇게 함으로써 쿼리 계획 과정 자체가 더 빨라지고 쿼리 계획이 필요한 파일만 스캔하므로 훨씬 빠르게 실행될 수 있음  사용 용이(Easy to Use): 파티셔닝 같은 기술의 이점을 얻으려면, 최종 사용자는 테이블의 물리적 구조를 알 필요가 없어야함, 테이블은 사용자에게 자연스럽고 직관적인 쿼리를 통해 파티셔닝의 이점을 제공할 수 있어야하고 이미 필터링하고 있는 컬럼(e.g. 타임스탬프 컬럼)에서 파생된 추가적인 파티션 컬럼(e.g. month 컬럼)을 또 필터링하는 방식에 의존하지 않아야 함.  진화/변경 가능함 (Evolvability): Hive 테이블의 스키마를 수정하면 안전하지 않은 트랜잭션 발생 가능, 테이블의 파티셔닝 방식을 변경할 때에는 전체 테이블 다시 작성 필요, 테이블은 스키마와 파티셔닝의 방식을 안전하게 변경할 수 있어야 하며 이를 위해 전체 재작성할 필요가 없어야 함.  확장성 (Scalability): 위 요구사항들이 페타바이트급 데이터에서 달성 가능해야 함. (Iceberg는 넷플릭스에서 만들었음!)            Apache Iceberg: The Definitive Guide, O’Reilly Media, 2024. p.16 &#8617;      ",
"tags": ["iceberg", "bigdata", "datalake"]
},

{
"title": "Welcome!",
"url": "/welcome/",
"content": "환영해요!",
"tags": []
}

]